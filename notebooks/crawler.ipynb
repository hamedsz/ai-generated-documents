# %% [markdown]
# # Company Career Page Crawler Notebook
# 
# **Purpose**: Crawl company websites, detect career pages, and store job openings

# %%
# Install requirements
%pip install requests beautifulsoup4 sqlalchemy pandas matplotlib

# %% [markdown]
# ## 1. Database Setup

# %%
from sqlalchemy import create_engine, Column, Integer, String, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import json

Base = declarative_base()

class Company(Base):
    __tablename__ = 'companies'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    domain = Column(String)
    career_page = Column(String)

class Job(Base):
    __tablename__ = 'jobs'
    id = Column(Integer, primary_key=True)
    company_id = Column(Integer)
    title = Column(String)
    description = Column(Text)
    url = Column(String)
    location = Column(String)

# Initialize database
engine = create_engine('sqlite:///jobs.db')
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

# %% [markdown]
# ## 2. Crawler Implementation

# %%
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import pandas as pd

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
HEADERS = {'User-Agent': USER_AGENT}
CAREER_KEYWORDS = ['career', 'job', 'join-us', 'opportunities', 'vacancies']

def find_career_page(domain):
    try:
        response = requests.get(f"https://{domain}", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for link in soup.find_all('a', href=True):
            href = link['href'].lower()
            text = link.get_text().lower()
            if any(keyword in href or keyword in text for keyword in CAREER_KEYWORDS):
                return urljoin(response.url, link['href'])
        
        for path in ['/careers', '/jobs', '/join-us']:
            try:
                response = requests.get(f"https://{domain}{path}", headers=HEADERS, timeout=10)
                if response.status_code == 200:
                    return response.url
            except:
                continue
                
    except Exception as e:
        print(f"Error processing {domain}: {str(e)}")
    return None

def extract_jobs(career_url):
    try:
        response = requests.get(career_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        jobs = []
        for script in soup.find_all('script', type='application/ld+json'):
            try:
                data = json.loads(script.string)
                if isinstance(data, list):
                    data = data[0]
                if data.get('@type') == 'JobPosting':
                    jobs.append({
                        'title': data.get('title'),
                        'description': data.get('description'),
                        'url': data.get('url'),
                        'location': data.get('jobLocation', {}).get('address', {}).get('addressLocality')
                    })
            except:
                continue
        
        if not jobs:
            for job_element in soup.find_all(['div', 'li'], class_=lambda x: x and 'job' in x.lower()):
                title = job_element.find(class_=lambda x: x and 'title' in x.lower())
                desc = job_element.find(class_=lambda x: x and 'description' in x.lower())
                jobs.append({
                    'title': title.get_text().strip() if title else 'N/A',
                    'description': desc.get_text().strip() if desc else 'N/A',
                    'url': urljoin(career_url, job_element.find('a')['href']) if job_element.find('a') else career_url,
                    'location': 'N/A'
                })
        
        return jobs
    except Exception as e:
        print(f"Error extracting jobs from {career_url}: {str(e)}")
        return []

# %% [markdown]
# ## 3. Run Crawler

# %%
def crawl_companies(company_list):
    session = Session()
    
    results = []
    for name, domain in company_list:
        print(f"\nüîç Crawling {name} ({domain})")
        career_url = find_career_page(domain)
        if career_url:
            print(f"‚úÖ Found career page: {career_url}")
            jobs = extract_jobs(career_url)
            if jobs:
                # Save to DB
                company = Company(name=name, domain=domain, career_page=career_url)
                session.add(company)
                session.commit()
                
                for job in jobs:
                    new_job = Job(
                        company_id=company.id,
                        title=job['title'],
                        description=job['description'],
                        url=job['url'],
                        location=job['location']
                    )
                    session.add(new_job)
                session.commit()
                
                results.append({
                    'Company': name,
                    'Domain': domain,
                    'Jobs Found': len(jobs),
                    'Career Page': career_url
                })
                print(f"üíº Saved {len(jobs)} jobs")
            else:
                print("‚ö†Ô∏è No jobs found")
        else:
            print("‚ùå No career page found")
        time.sleep(1)
    
    return pd.DataFrame(results)

# %% [markdown]
# ## 4. Input Companies to Crawl

# %%
# Edit this list of companies
companies_to_crawl = [
    ('Google', 'google.com'),
    ('Microsoft', 'microsoft.com'),
    ('Amazon', 'amazon.com'),
    ('Netflix', 'netflix.com'),
    ('GitHub', 'github.com')
]

results_df = crawl_companies(companies_to_crawl)

# %% [markdown]
# ## 5. View Results

# %%
# Show results table
from IPython.display import display
display(results_df)

# Show job distribution
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
results_df.set_index('Company')['Jobs Found'].plot(kind='bar', color='skyblue')
plt.title('Jobs Found per Company')
plt.ylabel('Number of Jobs')
plt.xticks(rotation=45)
plt.show()

# %% [markdown]
# ## 6. View Database Contents

# %%
# Show companies in database
session = Session()
companies = pd.read_sql(session.query(Company).statement, session.bind)
jobs = pd.read_sql(session.query(Job).statement, session.bind)

print("\nüì¶ Companies in Database:")
display(companies)

print("\nüëî Jobs in Database:")
display(jobs.head())

# %% [markdown]
# ### To Use:
# 1. Run all cells
# 2. Modify the `companies_to_crawl` list
# 3. Re-run cells from section 4 onward
# 
# **Note**: Be respectful with crawling frequency and check websites' robots.txt!

# %% [markdown]
# ‚ö†Ô∏è **Important Considerations**:
# - Add delays between requests (already included)
# - Some websites might block scrapers
# - Career page detection isn't perfect
# - Many sites require JavaScript rendering (consider adding Selenium/Playwright)
